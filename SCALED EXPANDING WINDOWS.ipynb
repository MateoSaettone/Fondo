{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c507e2f-ef6b-47ad-915e-45ede142c008",
   "metadata": {},
   "source": [
    "### **LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b44345-39af-4b3f-85a8-c0e1e9a02e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97df316-78a9-40d9-b9eb-3ac57c7ee21c",
   "metadata": {},
   "source": [
    "### **DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff6d2d-b50a-4908-bc47-870b78f9348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import historical data from yahoo finance API\n",
    "# Prices per industry (Tickers)\n",
    "tickers = [\n",
    "\n",
    "    # Target of the model\n",
    "    \"SPY\", \n",
    "    \n",
    "    # Technology\n",
    "    \"AAPL\", \"MSFT\", \"NVDA\", \"GOOGL\", \"INTC\",\n",
    "    \"CSCO\", \"TXN\", \"IBM\", \"ORCL\", \"QCOM\", \"AMZN\",\n",
    "    \"TSLA\", \"META\",\n",
    "\n",
    "    # Financials\n",
    "    \"GS\", \"BAC\", \"C\", \"WFC\", \"MS\", \"AXP\", \"BRK-B\", \n",
    "    \"V\", \"MA\",\"JPM\",\n",
    "\n",
    "    # Healthcare\n",
    "    \"UNH\", \"JNJ\", \"PFE\", \"LLY\", \"ABBV\", \"MRK\", \n",
    "    \"AMGN\",\"MDT\", \"CI\", \"CVS\",\n",
    "\n",
    "    # Index(s)\n",
    "    \"^OEX\", \"NDAQ\"\n",
    "]\n",
    "\n",
    "# Empty dictionary to keep tickers\n",
    "data_dict = {}\n",
    "\n",
    "# Import historical data for each ticker\n",
    "for ticker in tickers:\n",
    "    stock_data = yf.Ticker(ticker).history(start = \"2009-01-01\", end = \"2026-01-01\", interval = \"1d\", auto_adjust = False)[['Close']]\n",
    "    stock_data.index = stock_data.index.tz_localize(None)\n",
    "    stock_data.index.name = 'Date'\n",
    "    stock_data.rename(columns = {'Close': ticker}, inplace = True)\n",
    "    data_dict[ticker] = stock_data\n",
    "\n",
    "# Individual dataframe for each stock\n",
    "for ticker in tickers:\n",
    "    globals()[ticker] = data_dict[ticker]\n",
    "    globals()[ticker] = globals()[ticker][[ticker]]\n",
    "\n",
    "# Merge dataframes \n",
    "SPY_combined = SPY.copy()\n",
    "for ticker in tickers:\n",
    "    if ticker != \"SPY\":  \n",
    "        SPY_combined = SPY_combined.join(globals()[ticker], how = \"inner\") \n",
    "        \n",
    "# Adjust dataset\n",
    "df = SPY_combined\n",
    "df.rename(columns = {'SPY': 'TARGET'}, inplace = True)\n",
    "df = df.dropna()\n",
    "model_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a415b763-e450-43c0-80c2-6edc28d9614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48475a1-a565-4c84-92fd-b4ef7264f0fa",
   "metadata": {},
   "source": [
    "### **DATA ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b08e74-c87a-4e02-8e9b-7aa90816f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between S&P and other variables\n",
    "correlation_with_sp = df.corr()['TARGET']\n",
    "corr = pd.DataFrame(correlation_with_sp)\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7660e8-31a3-49d0-b245-838185eb0a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix \n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Heatmap for the correlation matrix\n",
    "plt.figure(figsize=(20, 18)) \n",
    "sns.heatmap(corr_matrix, annot = True, cmap = 'coolwarm', fmt = '.2f', linewidths = 0.5)\n",
    "plt.title('Heatmap of Correlation between Columns in the DataFrame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4cb013-a353-46ad-8f9d-bb55aada9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prices over-time\n",
    "for column in df.columns:\n",
    "    plt.figure()  \n",
    "    plt.plot(df[column])\n",
    "    plt.xlabel('Index')  \n",
    "    plt.ylabel(column)   \n",
    "    plt.title(f'Plot of {column}')  \n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e73730-c2c4-4115-898e-3201aaa14454",
   "metadata": {},
   "source": [
    "### **WEIGHTED INDUSTRIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689e162-86f4-477a-a293-a4a6048ddc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticker for SPY\n",
    "tickers = [\"SPY\"]\n",
    "\n",
    "# Dictionary to store data\n",
    "data_dict = {}\n",
    "\n",
    "# Import SPY data\n",
    "for ticker in tickers:\n",
    "    stock_data = yf.Ticker(ticker).history(start=\"2013-01-02\", end=\"2026-01-01\", interval=\"1d\", auto_adjust = False)\n",
    "    stock_data.index = stock_data.index.tz_localize(None)\n",
    "    stock_data.index.name = 'Date'\n",
    "    data_dict[ticker] = stock_data\n",
    "\n",
    "# Combine SPY data\n",
    "SPY_combined = data_dict[\"SPY\"]\n",
    "\n",
    "# Prepare dataset and remove nulls\n",
    "target = SPY_combined \n",
    "target = target.dropna()  \n",
    "\n",
    "# Drop unnecessary columns and create TARGET\n",
    "target['TARGET'] = (target['Close'] / target['Open']) - 1\n",
    "target = target.drop(columns=['Dividends', 'Volume', 'Stock Splits', \n",
    "                              'Capital Gains', 'Low', 'High', 'Open', 'Close'\n",
    "                             ])\n",
    "target = target.drop(df.index[0])\n",
    "\n",
    "# Convert prices to percentage changes\n",
    "df = df.pct_change()\n",
    "df = df.dropna()\n",
    "df = df * 100\n",
    "\n",
    "# Create TARGET\n",
    "df[['TARGET']] = target[['TARGET']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a94d1-39c5-47b8-8589-661bf3c45250",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303f07c-0a4d-42bf-9f2d-75a22fb68c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define industries\n",
    "industries = {\n",
    "    \"tech\": [\"AAPL\", \"MSFT\", \"NVDA\", \"GOOGL\", \"INTC\", \"CSCO\", \"TXN\", \"IBM\", \"ORCL\", \"QCOM\", \"AMZN\", \"TSLA\", \"META\"],\n",
    "    \"fin\": [\"GS\", \"BAC\", \"C\", \"WFC\", \"MS\", \"AXP\", \"BRK-B\", \"V\", \"MA\", \"JPM\"],\n",
    "    \"health\": [\"UNH\", \"JNJ\", \"PFE\", \"LLY\", \"ABBV\", \"MRK\", \"AMGN\", \"MDT\", \"CI\", \"CVS\"]\n",
    "}\n",
    "\n",
    "# Function to calculate rolling beta for multiple stocks\n",
    "def calculate_rolling_beta(industry_tickers, df):\n",
    "    industry_data = {\"Date\": df.index}\n",
    "    for ticker in industry_tickers:\n",
    "        stock_corr = df[[ticker, \"TARGET\"]].dropna()\n",
    "        n_records = len(stock_corr)\n",
    "        betas = []\n",
    "\n",
    "        # Calcular la beta en ventanas de 1400 días\n",
    "        for i in range(n_records - 1400 + 1):\n",
    "            X = stock_corr.iloc[i:i+1400][ticker]\n",
    "            Y = stock_corr.iloc[i:i+1400][\"TARGET\"]\n",
    "            cov_XY = X.cov(Y)\n",
    "            var_X = X.var()\n",
    "            beta = cov_XY / var_X\n",
    "            betas.append(beta)\n",
    "\n",
    "        # Crear DataFrame con las betas móviles\n",
    "        beta_series = pd.Series([np.nan] * 1399 + betas, index=stock_corr.index, name=f\"{ticker}_BETA\")\n",
    "        industry_data[f\"{ticker}_BETA\"] = beta_series\n",
    "\n",
    "    return pd.DataFrame(industry_data).set_index(\"Date\")\n",
    "\n",
    "# Calculate rolling betas for each industry\n",
    "BETA_tech = calculate_rolling_beta(industries[\"tech\"], df).dropna()\n",
    "BETA_fin = calculate_rolling_beta(industries[\"fin\"], df).dropna()\n",
    "BETA_health = calculate_rolling_beta(industries[\"health\"], df).dropna()\n",
    "\n",
    "# Normalize betas by dividing by the sum of all betas for the respective industry\n",
    "def normalize_betas(beta_df):\n",
    "    beta_df['SUM_BETA'] = beta_df.sum(axis=1)\n",
    "    for column in beta_df.columns[:-1]:  \n",
    "        beta_df[column] = beta_df[column] / beta_df['SUM_BETA']\n",
    "    beta_df = beta_df.drop(columns=['SUM_BETA'])\n",
    "    return beta_df\n",
    "\n",
    "# Normalize each industry's betas\n",
    "BETA_tech_normalized = normalize_betas(BETA_tech)\n",
    "BETA_fin_normalized = normalize_betas(BETA_fin)\n",
    "BETA_health_normalized = normalize_betas(BETA_health)\n",
    "\n",
    "# Multiply normalized betas by stock prices\n",
    "def update_betas_with_prices(beta_df, price_data):\n",
    "    updated_beta_df = beta_df.copy()\n",
    "    for column in beta_df.columns:\n",
    "        ticker = column.replace(\"_BETA\", \"\")\n",
    "        if ticker in price_data.columns:\n",
    "            updated_beta_df[column] = beta_df[column] * price_data[ticker]\n",
    "    updated_beta_df[\"SUM\"] = updated_beta_df.sum(axis=1)\n",
    "    return updated_beta_df\n",
    "\n",
    "# Apply stock prices to normalized betas\n",
    "weighted_tech = update_betas_with_prices(BETA_tech_normalized, SPY_combined)\n",
    "weighted_fin = update_betas_with_prices(BETA_fin_normalized, SPY_combined)\n",
    "weighted_health = update_betas_with_prices(BETA_health_normalized, SPY_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d3e84-05c4-4953-9eaf-46294c156e8e",
   "metadata": {},
   "source": [
    "### **MODELING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c62a7-eb2b-45d5-9ea7-6c1f028aa211",
   "metadata": {},
   "source": [
    "#### **1| SETTING SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae7da73-4021-48bc-8922-333a2c6e2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating final dataset\n",
    "regression = pd.DataFrame({\n",
    "    \"TARGET\": model_df[\"TARGET\"],\n",
    "    \"TECH_INDUSTRY\": weighted_tech[\"SUM\"],\n",
    "    \"FIN_INDUSTRY\": weighted_fin[\"SUM\"],\n",
    "    \"HEALTH_INDUSTRY\": weighted_health[\"SUM\"],\n",
    "    \"OEX\": model_df[\"^OEX\"],\n",
    "    \"NDX\": model_df[\"NDAQ\"]\n",
    "})\n",
    "\n",
    "# Creating Target \n",
    "regression['TARGET'] = regression['TARGET'].shift(-1)\n",
    "regression = regression.dropna()\n",
    "\n",
    "# Adjusting Dataframe\n",
    "regression = regression.dropna()\n",
    "regression = regression.pct_change()\n",
    "regression = regression.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a01d9-d694-4827-8e45-17083fa9034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2302b4-5bd5-46be-a4d7-51c1014a3ad4",
   "metadata": {},
   "source": [
    "#### **2| INCREMENTAL REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a6024-f45d-4596-9849-7a64f7700921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and Y \n",
    "X = regression.drop(columns=['TARGET'])\n",
    "Y = regression['TARGET']\n",
    "\n",
    "# Add intercept to X\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# List to store betas\n",
    "betas = []\n",
    "\n",
    "# Train model with first 1429 records \n",
    "initial_data = regression.iloc[:1429]  \n",
    "X_initial = initial_data.drop(columns=['TARGET'])\n",
    "Y_initial = initial_data['TARGET']\n",
    "X_initial = sm.add_constant(X_initial)\n",
    "\n",
    "# OLS regression for initial data\n",
    "model_initial = sm.OLS(Y_initial, X_initial).fit()\n",
    "\n",
    "# Store initial coefficients\n",
    "betas.append(model_initial.params)\n",
    "\n",
    "# Recalculate betas for each new record\n",
    "for i in range(1429, len(regression)):  \n",
    "    # Subset data up to record i\n",
    "    current_data = regression.iloc[:i+1]\n",
    "    X_current = current_data.drop(columns=['TARGET'])\n",
    "    Y_current = current_data['TARGET']\n",
    "    X_current = sm.add_constant(X_current)\n",
    "\n",
    "    # Create logarithmic weights using log(x + 1)\n",
    "    indices = np.arange(1, len(current_data) + 1) \n",
    "    weights = np.log1p(indices)  \n",
    "\n",
    "    # OLS regression for current data with weights\n",
    "    model_current = sm.WLS(Y_current, X_current, weights=weights).fit()  \n",
    "    \n",
    "    # Store new coefficients\n",
    "    betas.append(model_current.params)\n",
    "\n",
    "# Convert betas to DataFrame\n",
    "betas_df = pd.DataFrame(betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1c91e-8db2-4e6e-82b9-3d3918ea39c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4220969-3741-45d0-878d-72f9c01504df",
   "metadata": {},
   "source": [
    "#### **3| PREDICTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca1f1b-5f8e-4c6f-9538-806d138771b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select last 200 rows\n",
    "regression = regression.iloc[-201:]\n",
    "\n",
    "# Reset index and convert to column\n",
    "regression_reset = regression.reset_index()\n",
    "\n",
    "# Add 'const' column from betas_df\n",
    "regression_reset['const'] = betas_df['const']\n",
    "\n",
    "# Get common columns\n",
    "common_columns = regression_reset.columns.intersection(betas_df.columns)\n",
    "\n",
    "# Multiply common columns\n",
    "for col in common_columns:\n",
    "    regression_reset[col] = regression_reset[col] * betas_df[col].values\n",
    "\n",
    "# Set 'Date' as index\n",
    "regression_reset.set_index('Date', inplace=True)\n",
    "\n",
    "# Drop missing values\n",
    "regression_reset = regression_reset.dropna()\n",
    "\n",
    "# Update regression\n",
    "regression = regression_reset\n",
    "\n",
    "# Remove the 'TARGET' column\n",
    "regression.drop(columns=['TARGET'], inplace=True)\n",
    "\n",
    "# Create 'PRED' column as the sum of all columns\n",
    "regression['PRED'] = regression.sum(axis=1)\n",
    "\n",
    "# Keep only the 'PRED' column\n",
    "regression = regression[['PRED']]\n",
    "regresssion = regression.dropna()\n",
    "\n",
    "# Create empty row with index '2025-01-16'\n",
    "regression.loc[pd.to_datetime('2025-01-17')] = None\n",
    "\n",
    "# Shift 'PRED_SAME_DATE' by 1\n",
    "regression['PRED'] = regression['PRED'].shift(1)\n",
    "\n",
    "# Drop NA values\n",
    "regression = regression.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bc02b-b9f1-4b80-8ed2-4bffda7207fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36dc79-9167-4bdf-b66c-d9f5b30d2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression.to_csv(\"PREDICTIONS.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
